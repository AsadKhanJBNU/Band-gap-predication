{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json,pickle\n",
    "from collections import OrderedDict\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles\n",
    "import networkx as nx\n",
    "\n",
    "from random import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from Data_Preprocessing.Graph_Data import Molecule_data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from models.AttentiveFPModel import AttentiveFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:3\"\n",
    "    print(\"cuda:3\")\n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "    print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(model, device, loader):\n",
    "    model.eval()\n",
    "    total_preds = torch.Tensor()\n",
    "    total_labels = torch.Tensor()\n",
    "    #print('Make prediction for {} samples...'.format(len(loader1.dataset)))\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data.x.float(), data.edge_index,data.batch)\n",
    "            total_preds = torch.cat((total_preds, output.cpu()), 0)\n",
    "            total_labels = torch.cat((total_labels, data.y.view(-1, 1).cpu()), 0)\n",
    "    return total_labels.numpy().flatten(),total_preds.numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 17\n",
    "processed_data_file_test1 = 'data/processed/' + 'test_data_set_fold_'+str(1)+'.pt'\n",
    "processed_data_file_train1 = 'data/processed/' + 'train_data_set_fold_'+str(1)+'.pt'\n",
    "processed_data_file_test2 = 'data/processed/'  + 'test_data_set_fold_'+str(2)+'.pt'\n",
    "\n",
    "if ((not os.path.isfile(processed_data_file_test1)) or (not os.path.isfile(processed_data_file_test2))):\n",
    "        print('please run create_data.py to prepare data in pytorch format!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data1 = Molecule_data(root='data', dataset='test_data_set_fold_'+str(1),y=None,smile_graph=None,smiles=None)\n",
    "test_data2 = Molecule_data(root='data', dataset='test_data_set_fold_'+str(2),y=None,smile_graph=None,smiles=None)\n",
    "\n",
    "test1_loder   = DataLoader(test_data1,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "test2_loder  = DataLoader(test_data2,batch_size=TRAIN_BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test1_loder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentiveFP(in_channels=114, hidden_channels=292, out_channels=1,\n",
    "                    num_layers=3, num_timesteps=2,\n",
    "                    dropout=0.047352327938708194).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = 'saved_models/model_' +  str(1) +  '.model'\n",
    "model.load_state_dict(torch.load(model_file_name))\n",
    "test_G,test_P = predicting(model, device, test1_loder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_G.shape,test_P.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_error(test_G,test_P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, metrics, decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "fig = plt.figure()\n",
    "i = 1\n",
    "for fold in range (5):\n",
    "   \n",
    "    test_data = Molecule_data(root='data', dataset='test_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = 17\n",
    "    test_loder  = DataLoader(test_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    \n",
    "    model = AttentiveFP(in_channels=114, hidden_channels=292, out_channels=1,\n",
    "                    num_layers=3, num_timesteps=2,\n",
    "                    dropout=0.047352327938708194).to(device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model_file_name = 'saved_models/model_' +  str(fold) +  '.model'\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_file_name))\n",
    "    true,prediction = predicting(model, device, test_loder)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = metrics.r2_score(true, prediction)\n",
    "    scores.append(score)\n",
    "    \n",
    "    plt.scatter(prediction, true, lw=2, alpha=0.5, \n",
    "                label='Fold %d (R2 = %0.2f)' % (i,score))\n",
    "    i = i+1\n",
    "plt.plot([min(prediction),max(true)], [min(prediction),max(true)], \n",
    "         linestyle='--', lw=2, color='black')\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('Attention FP 5-Fold Validation')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.savefig('5FoldsR2.png', dpi=400,transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "true_val = []\n",
    "pred_val = []\n",
    "fig = plt.figure()\n",
    "i = 1\n",
    "for fold in range (5):\n",
    "   \n",
    "    test_data = Molecule_data(root='data', dataset='test_data_set_fold_'+str(fold),y=None,smile_graph=None,smiles=None)\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = 17\n",
    "    test_loder  = DataLoader(test_data,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n",
    "    \n",
    "    model = AttentiveFP(in_channels=114, hidden_channels=292, out_channels=1,\n",
    "                    num_layers=3, num_timesteps=2,\n",
    "                    dropout=0.047352327938708194).to(device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model_file_name = 'saved_models/model_' +  str(fold) +  '.model'\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_file_name))\n",
    "    true,prediction = predicting(model, device, test_loder)\n",
    "    true_val.append(true)\n",
    "    pred_val.append(prediction)\n",
    "    score = mean_absolute_error(true,prediction)\n",
    "    scores.append(score)\n",
    "    \n",
    "    plt.scatter(prediction, true, lw=2, alpha=0.5, \n",
    "                label='Fold %d (MAE = %0.2f)' % (i,score))\n",
    "    i = i+1\n",
    "\n",
    "plt.plot([min(prediction),max(true)], [min(prediction),max(true)], \n",
    "         linestyle='--', lw=2, color='black')\n",
    "plt.xlabel('Ground truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.legend()\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.savefig('5Folds.png', dpi=400,transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae=[]\n",
    "R_rmse=[]\n",
    "for i in range(5):\n",
    "    Mmae = mean_absolute_error(true_val[i],pred_val[i])\n",
    "    MSE = mean_squared_error(true_val[i], pred_val[i])\n",
    "    RMSE = math.sqrt(MSE)\n",
    "    mae.append(Mmae)\n",
    "    R_rmse.append(RMSE)\n",
    "    \n",
    "avg_mae = sum(mae)/5\n",
    "avg_rmse = sum(R_rmse)/5\n",
    "\n",
    "print(avg_mae,avg_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(test_G, test_P, c='crimson')\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "\n",
    "p1 = max(max(test_P), max(test_G))\n",
    "p2 = min(min(test_P), min(test_G))\n",
    "plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "plt.title('Test prediction Value', fontsize=15)\n",
    "plt.xlabel('True Values', fontsize=10)\n",
    "plt.ylabel('Predictions', fontsize=10)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
